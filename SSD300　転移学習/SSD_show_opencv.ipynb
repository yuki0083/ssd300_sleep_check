{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # OpenCVライブラリ\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import DataTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了：学習済みの重みをロードしました\n"
     ]
    }
   ],
   "source": [
    "from utils.ssd_model import SSD\n",
    "\n",
    "voc_classes = ['safe', 'caution']\n",
    "\n",
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes': 3,  # 背景クラスを含めた合計クラス数\n",
    "    'input_size': 300,  # 画像の入力サイズ\n",
    "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n",
    "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
    "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
    "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "# SSDネットワークモデル\n",
    "net = SSD(phase=\"inference\", cfg=ssd_cfg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# SSDの学習済みの重みを設定(ここは場合によって変更)\n",
    "net_weights = torch.load('./weights/fine_tuning_32batch/ssd300_fine260.pth',\n",
    "                         map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "#net_weights = torch.load('./weights/ssd300_mAP_77.43_v2.pth',\n",
    "#                         map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "net.load_state_dict(net_weights)\n",
    "\n",
    "print('ネットワーク設定完了：学習済みの重みをロードしました')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測と表示を行うクラスを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDPredictShow():\n",
    "    \"\"\"SSDでの予測と画像の表示をまとめて行うクラス\"\"\"\n",
    "\n",
    "    def __init__(self, eval_categories, net):\n",
    "        self.eval_categories = eval_categories  # クラス名\n",
    "        self.net = net  # SSDネットワーク\n",
    "\n",
    "        color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "        input_size = 300  # 画像のinputサイズを300×300にする\n",
    "        self.transform = DataTransform(input_size, color_mean)  # 前処理クラス\n",
    "        \n",
    "    def show(self, frame, data_confidence_level):\n",
    "        \"\"\"\n",
    "        物体検出の予測結果を表示をする関数。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_file_path:  str\n",
    "            画像のファイルパス\n",
    "        data_confidence_level: float\n",
    "            予測で発見とする確信度の閾値\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        なし。rgb_imgに物体検出結果が加わった画像が表示される。\n",
    "        \"\"\"\n",
    "        #SSDで予測させ(予測BBox, 予測ラベル, 確信度)を返す\n",
    "        frame, predict_bbox, pre_dict_label_index, scores = self.ssd_predict(frame, data_confidence_level) \n",
    "        #予測結果を表示\n",
    "        self.vis_bbox(frame, bbox=predict_bbox, label_index=pre_dict_label_index,\n",
    "                      scores=scores, label_names=self.eval_categories)\n",
    "\n",
    "    #def ssd_predict(self, image_file_path, data_confidence_level=0.5):\n",
    "    def ssd_predict(self, frame, data_confidence_level=0.3):\n",
    "        \n",
    "        \"\"\"\n",
    "        SSDで予測させる関数。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image_file_path:  strt\n",
    "            画像のファイルパス\n",
    "\n",
    "        dataconfidence_level: float\n",
    "            予測で発見とする確信度の閾値\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        rgb_img, true_bbox, true_label_index, predict_bbox, pre_dict_label_index, scores\n",
    "        \"\"\"\n",
    "\n",
    "        # rgbの画像データを取得\n",
    "        #img = frame\n",
    "        #img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
    "        height, width, channels = frame.shape  # 画像のサイズを取得\n",
    "        #rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)#BGR→RGB\n",
    "\n",
    "        # 画像の前処理\n",
    "        phase = \"val\"\n",
    "        img_transformed, boxes, labels = self.transform(\n",
    "            frame, phase, \"\", \"\")  # アノテーションが存在しないので\"\"にする。\n",
    "        img = torch.from_numpy(\n",
    "            img_transformed[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
    "        \n",
    "        # SSDで予測\n",
    "        self.net.eval()  # ネットワークを推論モードへ\n",
    "        x = img.unsqueeze(0)  # ミニバッチ化：torch.Size([1, 3, 300, 300])\n",
    "\n",
    "        detections = self.net(x)#予測結果torch.Size([batch_num, クラス数, 200, 5])\n",
    "\n",
    "        # detectionsの形は、torch.Size([1, 21, 200, 5])  ※200はtop_kの値\n",
    "\n",
    "        \n",
    "        \n",
    "        # confidence_levelが基準以上を取り出す\n",
    "        predict_bbox = []\n",
    "        pre_dict_label_index = []\n",
    "        scores = []\n",
    "        detections = detections.cpu().detach().numpy()#予測結果をnumpy形式に\n",
    "\n",
    "        # 条件以上の値を抽出\n",
    "        find_index = np.where(detections[:, 0:, :, 0] >= data_confidence_level)#確信度が(表示用の)閾値より大きいところはTrue(0:も0から最後まで)\n",
    "        detections = detections[find_index]\n",
    "        for i in range(len(find_index[1])):#find_index[1]は第一次元の数(クラス数)\n",
    "            # 抽出した物体数分ループを回す\n",
    "            if (find_index[1][i]) > 0:  #find_index[1][i]はクラスの番号\n",
    "                # 背景クラスでないもの\n",
    "                sc = detections[i][0]  # 確信度\n",
    "                bbox = detections[i][1:] * [width, height, width, height]#DBOXの規格化を解除してframeの大きさに合わせる\n",
    "                # find_indexはミニバッチ数、クラス、topのtuple\n",
    "                lable_ind = find_index[1][i]-1\n",
    "                # （注釈）\n",
    "                # 背景クラスが0なので1を引く\n",
    "\n",
    "                # 返り値のリストに追加\n",
    "                predict_bbox.append(bbox)\n",
    "                pre_dict_label_index.append(lable_ind)\n",
    "                scores.append(sc)\n",
    "\n",
    "        return  frame, predict_bbox, pre_dict_label_index, scores\n",
    "\n",
    "    def vis_bbox(self, img, bbox, label_index, scores, label_names):\n",
    "        \"\"\"\n",
    "        物体検出の予測結果を画像で表示させる関数。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rgb_img:rgbの画像\n",
    "            対象の画像データ\n",
    "        bbox: list\n",
    "            物体のBBoxのリスト\n",
    "        label_index: list\n",
    "            物体のラベルへのインデックス\n",
    "        scores: list\n",
    "            物体の確信度。\n",
    "        label_names: list\n",
    "            ラベル名の配列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        なし。rgb_imgに物体検出結果が加わった画像が表示される。\n",
    "        \"\"\"\n",
    "\n",
    "        # 枠の色の設定\n",
    "        num_classes = len(label_names)  # クラス数（背景のぞく）\n",
    "        #colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()\n",
    "\n",
    "        # 画像の表示\n",
    "        #plt.figure(figsize=(10, 10))\n",
    "        #plt.imshow(rgb_img)\n",
    "        #currentAxis = plt.gca()\n",
    "\n",
    "        # BBox分のループ\n",
    "        for i, bb in enumerate(bbox):\n",
    "\n",
    "            # 取り出したBBoxに対応するラベル名\n",
    "            label_name = label_names[label_index[i]]\n",
    "            #color = colors[label_index[i]]  # クラスごとに別の色の枠を与える\n",
    "\n",
    "            # 枠につけるラベル　例：person;0.72　\n",
    "            if scores is not None:\n",
    "                sc = scores[i]\n",
    "                display_txt = '%s: %.2f' % (label_name, sc)\n",
    "            else:\n",
    "                display_txt = '%s: ans' % (label_name)\n",
    "            \n",
    "            color = (0, 0, 0)\n",
    "            #色の設定\n",
    "            if label_index[i] == 1:\n",
    "                color = (0, 0, 255)\n",
    "            if label_index[i] == 0:\n",
    "                color = (0, 255, 0)\n",
    "\n",
    "            # 枠の座標\n",
    "            xy = (bb[0], bb[1])\n",
    "            width = bb[2] - bb[0]\n",
    "            height = bb[3] - bb[1]\n",
    "            #bb[0]=xmin, bb[1]=ymin, bb[2]=xmax, bb[3]=ymax\n",
    "\n",
    "            # 長方形を描画する\n",
    "            #currentAxis.add_patch(plt.Rectangle(\n",
    "            #   xy, width, height, fill=False, edgecolor=color, linewidth=2))\n",
    "\n",
    "            \n",
    "            #opevcvで長方形を描画\n",
    "            cv2.rectangle(img, (int(bb[0]), int(bb[1])), (int(bb[2]), int(bb[3])), color, 2)\n",
    "\n",
    "            # 長方形の枠の左上にラベルを描画する\n",
    "            #currentAxis.text(xy[0], xy[1], display_txt, bbox={\n",
    "             #                'facecolor': color, 'alpha': 0.5})\n",
    "                \n",
    "            # opencvで長方形の枠の左上にラベルを描画する\n",
    "            font = cv2.FONT_HERSHEY_PLAIN#フォントの設定\n",
    "            cv2.putText(img, display_txt, (int(bb[0]),int(bb[1])), font, 1, color,1, lineType=cv2.LINE_AA)#(文字, 座標, フォント, (太さ), 色, 線を綺麗に)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path ='C:\\\\Users\\\\yuki\\\\Desktop\\\\うつ伏せ発見プログラム\\\\data\\\\nwcam\\\\original\\\\val_data\\\\10588047_200122140112_L.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function VideoCapture.release>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#動画の読み込み\n",
    "#video_path = input('動画のパスを入力してください>')\n",
    "cap = cv2.VideoCapture(video_path)#動画の読み込み\n",
    "\n",
    "\n",
    "if cap.isOpened()== False:#動画が読み込めたか\n",
    "    sys.exit() #プログラムを終了\n",
    "\n",
    "ret, frame = cap.read()\n",
    "#cv2.imshow(\"video\", frame)\n",
    "#cv2.waitKey(0) # 何かしらの入力がされるまで待機 \n",
    "#入力されると次のdestroywindowに進みwindowが消える\n",
    "#cv2.destroyAllWindows()#役目を終えるとimgというwindowを壊す\n",
    "#if cv2.waitKey(30) == 27 :#30mms(FPS30より)待つ間に27(Escキー)が押されると動画からbreak\n",
    "#   cv2.destroyAllWindows()\n",
    "cap.release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function VideoCapture.release>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#動画の読み込み\n",
    "#video_path = input('動画のパスを入力してください>')\n",
    "cap = cv2.VideoCapture(video_path)#動画の読み込み\n",
    "\n",
    "\n",
    "if cap.isOpened()== False:#動画が読み込めたか\n",
    "    sys.exit() #プログラムを終了\n",
    "#FPS調整\n",
    "#import time\n",
    "#wait_time = 0.1\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read() #動画は１フレームの読み込みと表示の繰り返し\n",
    "    if ret == False:\n",
    "        break\n",
    "    #予測(SSDの出力)\n",
    "    ssd = SSDPredictShow(eval_categories=voc_classes, net=net)\n",
    "    # output : torch.Size([batch_num, 21, 200, 5])  (今回1枚だけなので、batch_num=1)\n",
    "#  =（batch_num、クラス、confのtop200、規格化されたBBoxの情報）\n",
    "#   規格化されたBBoxの情報（確信度、xmin, ymin, xmax, ymax）\n",
    "\n",
    "    #予測結果を含んだframeを返す\n",
    "    ssd.show(frame, data_confidence_level=0.2)#確信度の下限をいじると変わる\n",
    "    \n",
    "    cv2.imshow(\"video\", frame)\n",
    "\n",
    "    if cv2.waitKey(30) == 27 :#30mms(FPS30より)待つ間に27(Escキー)が押されると動画からbreak\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "cap.release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
